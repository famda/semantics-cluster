
services:

  # ---------------------------------------------------------------------------
  # HEAD profile – run on the head node machine:
  #   docker compose --profile head up -d
  # ---------------------------------------------------------------------------

  tailscale-head:
    image: tailscale/tailscale:latest
    profiles: [head]
    hostname: ray-head
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY}
      - TS_HOSTNAME=ray-head
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
      - TS_EXTRA_ARGS=--advertise-tags=tag:ray-cluster --accept-dns=true
    devices:
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - net_admin
      - sys_module
    volumes:
      - tailscale-head-state:/var/lib/tailscale
    # All services sharing this network namespace expose their ports here
    ports:
      - "6379:6379"    # Ray GCS
      - "8265:8265"    # Ray Dashboard
      - "9090:9090"    # Prometheus
      - "3000:3000"    # Grafana
      - "10000:10000"  # Azurite Blob
      - "5080:5080"    # Transcription API
    # restart: unless-stopped

  ray-head:
    build:
      context: .
      dockerfile: Dockerfile.ray
    image: semantics-ray:latest
    profiles: [head]
    network_mode: "service:tailscale-head"
    depends_on:
      - tailscale-head
    user: "0"
    shm_size: 8g
    environment:
      # Server-side: Ray head health-checks these via localhost (same namespace)
      - RAY_PROMETHEUS_HOST=http://localhost:9090
      - RAY_GRAFANA_HOST=http://localhost:3000
      # Browser-side: Ray Dashboard embeds Grafana panels in iframes
      - RAY_GRAFANA_IFRAME_HOST=${RAY_GRAFANA_IFRAME_HOST:-http://localhost:3000}
      - AZURITE_CONN_STR=${AZURITE_CONN_STR}
      - ORT_LOG_LEVEL=3
      - RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
      - HF_HUB_DISABLE_TELEMETRY=1
    volumes:
      - ray-metrics:/tmp/ray
      - ./src/jobs:/opt/jobs:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Wait for Tailscale interface, get the Tailscale IP, then start Ray
    # bound to that IP so workers on other machines can reach the head.
    command: >
      bash -c "
        echo 'Waiting for Tailscale interface...' &&
        until python3 -c \"import fcntl,socket,struct;s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM);print(socket.inet_ntoa(fcntl.ioctl(s.fileno(),0x8915,struct.pack('256s',b'tailscale0'))[20:24]))\" 2>/dev/null; do sleep 2; done &&
        TAILSCALE_IP=$$(python3 -c \"import fcntl,socket,struct;s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM);print(socket.inet_ntoa(fcntl.ioctl(s.fileno(),0x8915,struct.pack('256s',b'tailscale0'))[20:24]))\") &&
        echo \"Tailscale IP: $$TAILSCALE_IP\" &&
        ray start --head --node-ip-address=$$TAILSCALE_IP --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265 --metrics-export-port=8079 --block"
    # restart: unless-stopped

  # ---------------------------------------------------------------------------
  # NODE profile – run on each worker machine:
  #   docker compose --profile node up -d
  # Set in .env: TS_HOSTNAME=ray-worker-<N>  RAY_HEAD_ADDRESS=ray-head:6379
  # ---------------------------------------------------------------------------

  tailscale-node:
    image: tailscale/tailscale:latest
    profiles: [node]
    hostname: ${TS_HOSTNAME:-ray-worker}
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY}
      - TS_HOSTNAME=${TS_HOSTNAME:-ray-worker}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
      - TS_EXTRA_ARGS=--advertise-tags=tag:ray-cluster --accept-dns=true
    devices:
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - net_admin
      - sys_module
    volumes:
      - tailscale-node-state:/var/lib/tailscale
    # restart: unless-stopped

  ray-node:
    build:
      context: .
      dockerfile: Dockerfile.ray
    image: semantics-ray:latest
    profiles: [node]
    network_mode: "service:tailscale-node"
    depends_on:
      - tailscale-node
    shm_size: 8g
    environment:
      - RAY_HEAD_ADDRESS=${RAY_HEAD_ADDRESS:-ray-head:6379}
      - ORT_LOG_LEVEL=3
      - RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
      - HF_HUB_DISABLE_TELEMETRY=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      bash -c "
        echo 'Waiting for Tailscale interface...' &&
        until python3 -c \"import fcntl,socket,struct;s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM);print(socket.inet_ntoa(fcntl.ioctl(s.fileno(),0x8915,struct.pack('256s',b'tailscale0'))[20:24]))\" 2>/dev/null; do sleep 2; done &&
        TAILSCALE_IP=$$(python3 -c \"import fcntl,socket,struct;s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM);print(socket.inet_ntoa(fcntl.ioctl(s.fileno(),0x8915,struct.pack('256s',b'tailscale0'))[20:24]))\") &&
        echo \"Tailscale IP: $$TAILSCALE_IP\" &&
        echo 'Waiting for ray-head:6379 to be reachable via Tailscale...' &&
        until bash -c 'echo >/dev/tcp/ray-head/6379' 2>/dev/null; do
          echo 'ray-head:6379 not yet reachable, retrying in 5s...'; sleep 5;
        done &&
        echo 'ray-head reachable, joining cluster...' &&
        ray start --address=$${RAY_HEAD_ADDRESS:-ray-head:6379} --node-ip-address=$$TAILSCALE_IP --metrics-export-port=8079 --block"

  # ---------------------------------------------------------------------------
  # Storage & API – all in the tailscale-head network namespace
  # ---------------------------------------------------------------------------

  semantics.api:
    image: ${DOCKER_REGISTRY-}semantics-api
    build:
      context: .
      dockerfile: src/Semantics.Api/Dockerfile
    profiles: [head]
    network_mode: "service:tailscale-head"
    depends_on:
      - azurite
      - ray-head
    environment:
      - ASPNETCORE_URLS=http://0.0.0.0:5080
      - ASPNETCORE_ENVIRONMENT=Development
      - RAY_DASHBOARD_URL=http://localhost:8265
      - AZURITE_CONN_STR=${AZURITE_CONN_STR}

  # ---------------------------------------------------------------------------
  # Monitoring stack – all in the tailscale-head network namespace so they can
  # reach each other at localhost and scrape worker metrics via Tailscale IPs.
  # ---------------------------------------------------------------------------

  azurite:
    image: mcr.microsoft.com/azure-storage/azurite
    profiles: [head]
    network_mode: "service:tailscale-head"
    depends_on:
      - tailscale-head
    volumes:
      - azurite-data:/data
    command: azurite-blob --blobHost 0.0.0.0 --blobPort 10000 -l /data --skipApiVersionCheck
    # restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    profiles: [head]
    network_mode: "service:tailscale-head"
    volumes:
      - ./.config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    depends_on:
      - tailscale-head
    # restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    profiles: [head]
    network_mode: "service:tailscale-head"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GF_ADMIN_PASSWORD:-changeme}
      - GF_SERVER_ROOT_URL=${RAY_GRAFANA_IFRAME_HOST:-http://localhost:3000}
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=none
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
    volumes:
      - ./.config/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana-data:/var/lib/grafana
      - ray-metrics:/tmp/ray:ro
    entrypoint: >
      /bin/bash -c "
        mkdir -p /var/lib/grafana/dashboards/ray &&
        echo 'Waiting for Ray to generate Grafana dashboards...' &&
        until ls /tmp/ray/session_*/metrics/grafana/dashboards/*.json >/dev/null 2>&1; do
          sleep 5;
        done &&
        cp /tmp/ray/session_*/metrics/grafana/dashboards/*.json /var/lib/grafana/dashboards/ray/ &&
        echo 'Ray dashboards copied, starting Grafana...' &&
        exec /run.sh"
    depends_on:
      - prometheus
      - ray-head
    # restart: unless-stopped

volumes:
  ray-metrics:
  prometheus-data:
  grafana-data:
  azurite-data:
  tailscale-head-state:
  tailscale-node-state:
